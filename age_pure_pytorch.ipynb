{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import copy\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler, swa_utils\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torchvision import transforms\n",
    "from diskcache import Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cache = Cache('.cache/diskcache')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATH = 'data/face_age'\n",
    "\n",
    "\n",
    "@cache.memoize()\n",
    "def imgpath_to_normalized_tensor(imgpath):\n",
    "        # makes a tensor, scales range to 0-1 and normalizes to same as imagenet\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "        img = normalize(transforms.PILToTensor()(Image.open(imgpath)).float()/255)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Ageset(Dataset):\n",
    "    def __init__(self, path, transforms = None, valid=False, split_pct = 0.3):\n",
    "        self.image_paths = list(Path(path).rglob(\"*.png\"))\n",
    "        random.seed(42)\n",
    "        random.shuffle(self.image_paths)\n",
    "        split_point = int(len(self)*0.3)\n",
    "        if valid:\n",
    "            self.image_paths = self.image_paths[:split_point]\n",
    "            print('len validation dataset', len(self.image_paths))\n",
    "        else:\n",
    "            self.image_paths = self.image_paths[split_point:]\n",
    "            print('len train dataset', len(self.image_paths))\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    # @functools.lru_cache(maxsize=None)\n",
    "    @classmethod # somehow this is needed for diskcache to work properly. Or define the function outside of the class\n",
    "    @cache.memoize()\n",
    "    def imgpath_to_normalized_tensor(cls,imgpath):\n",
    "            # makes a tensor, scales range to 0-1 and normalizes to same as imagenet\n",
    "            normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "            img = normalize(transforms.PILToTensor()(Image.open(imgpath)).float()/255)\n",
    "            return img\n",
    "            \n",
    "    def __getitem__(self,i):\n",
    "        if isinstance(i, slice):\n",
    "            return [self[n] for n,_ in enumerate(self.image_paths[i])]\n",
    "        \n",
    "        return (self.imgpath_to_normalized_tensor(self.image_paths[i]),\n",
    "                int(self.image_paths[i].parent.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tensor_dataset(path, max_len=None):\n",
    "    # this took 20 GB RAM max\n",
    "    image_paths = list(Path(path).rglob(\"*.png\"))\n",
    "    random.shuffle(image_paths)\n",
    "    if max_len:\n",
    "        image_paths = image_paths[:max_len]\n",
    "    \n",
    "    xs = torch.empty(len(image_paths), 3,200,200)\n",
    "    print('empty created')\n",
    "    for i, loc in enumerate(image_paths):\n",
    "        if i%1000==0: print(i)\n",
    "        xs[i] = imgpath_to_normalized_tensor(loc)\n",
    "    torch.save(xs,'data/input/xs')\n",
    "\n",
    "    print('done')\n",
    "\n",
    "    ys = torch.stack([torch.Tensor([int(Path(loc).parent.name)]) for loc in image_paths])\n",
    "    torch.save(ys,'data/input/ys')\n",
    "    print('done')\n",
    "    return None\n",
    "# construct_tensor_dataset(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeTensorDataset(TensorDataset):\n",
    "    def __init__(self, xs, ys, valid=False, split_pct=0.3):\n",
    "        length = len(xs)\n",
    "        split = int(xs.shape[0]*split_pct)\n",
    "        if valid:\n",
    "            super().__init__(xs[:split],ys[:split])\n",
    "        else:\n",
    "            super().__init__(xs[split:],ys[split:])\n",
    "\n",
    "    def __getitem__(self,x):\n",
    "        return super().__getitem__(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeResnet(nn.Module):\n",
    "    def __init__(self, size='18', feat_extract=False):\n",
    "        super().__init__()\n",
    "        resnet = 'torchvision.models.resnet'+size+'(pretrained=True)'\n",
    "        resnet = eval(resnet)\n",
    "        modules=list(resnet.children())[:-1]\n",
    "        self.resnet =nn.Sequential(*modules)\n",
    "\n",
    "        if feat_extract:\n",
    "            # with feature extraction we only train the linear layer and keep the resnet parameters fixed \n",
    "            for m in self.modules():\n",
    "                m.requires_grad_(False)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=512, out_features=1, bias=True)\n",
    "        nn.init.kaiming_normal_(self.fc.weight)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.resnet(x)\n",
    "        x = torch.flatten(out, 1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_size(dataset):\n",
    "    num_items = len(dataset)\n",
    "    img_dimensions = list(dataset[0][0].shape)\n",
    "    bytes_per_fp32 = 4\n",
    "    bytes_per_gb = 1024**3\n",
    "    size_in_gb = num_items * int(np.product(img_dimensions)) * bytes_per_fp32 / bytes_per_gb\n",
    "    print('items in dataset', num_items, 'img_dimensions', img_dimensions, 'size of ds in memory in gb:', size_in_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    best_loss = 1000000000\n",
    "    best_model = None\n",
    "    not_improve_count = 0\n",
    "    loss = {'train':[], 'val':[]}\n",
    "\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        print(f'Starting epoch {epoch}')\n",
    "        start_time = time.time()\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            total_loss = 0\n",
    "            for data in dls[phase]:\n",
    "                x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    pred = model(x)\n",
    "                    loss = loss_fn(y, pred)\n",
    "                    total_loss += loss * len(y)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        opt.step()\n",
    "                        opt.zero_grad()\n",
    "                        if SWA_ENABLED and epoch > SWA_START:\n",
    "                            swa_model.update_parameters(model)\n",
    "                            swa_sched.step()\n",
    "                        elif SCHED_ENABLED:\n",
    "                            sched.step(loss)\n",
    "                            writer.add_scalar('lr/scheduler', sched.get_last_lr()[0], epoch)\n",
    "                        writer.add_scalar('lr/optparamgroup0', opt.param_groups[0]['lr'], epoch)\n",
    "                writer.add_scalar('batchloss/'+phase, loss, epoch)\n",
    "            \n",
    "            writer.add_scalar('loss/'+phase, total_loss/len(dls[phase].dataset), epoch)\n",
    "    \n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            not_improve_count = 0\n",
    "        else:\n",
    "            not_improve_count += 1\n",
    "            if not_improve_count > 4:\n",
    "                print('early stopping!')\n",
    "                break\n",
    "\n",
    "        for x,y in val_set[:10]:\n",
    "            x = x.to(DEVICE)\n",
    "            print (model(x[None]),y)\n",
    "            \n",
    "        print(f\"loss after epoch {epoch} : {total_loss / len(dls['val'].dataset)}\")\n",
    "        writer.add_scalar('time', (time.time()-start_time)/60, epoch)\n",
    "\n",
    "\n",
    "    if SWA_ENABLED:\n",
    "        swa_model.to('cpu')\n",
    "        swa_utils.update_bn(train_dl, swa_model)\n",
    "        swa_model.to(DEVICE)\n",
    "        total_loss_train = 0\n",
    "        total_loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            for data in train_dl:\n",
    "                    x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                    total_loss_train += loss_fn(y, model(x)) * len(y)\n",
    "            writer.add_scalar('loss/train', total_loss_train/len(train_set), epoch+1)\n",
    "\n",
    "            for data in val_dl:\n",
    "                    x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                    total_loss_val += loss_fn(y, model(x)) * len(y)\n",
    "            writer.add_scalar('loss/val', total_loss_train/len(val_set), epoch+1)\n",
    "        swa_model.avg_fn=None\n",
    "        torch.save(swa_model,'data/output/'+'swa_model'+str(total_loss_val/len(val_set)))\n",
    "    torch.save(best_model,'data/output/'+'model'+str(best_loss/len(val_set)))\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len train dataset 13690\n",
      "items in dataset 4000 img_dimensions [3, 200, 200] size of ds in memory in gb: 1.7881393432617188\n",
      "len validation dataset 5866\n",
      "items in dataset 1000 img_dimensions [3, 200, 200] size of ds in memory in gb: 0.4470348358154297\n",
      "Starting epoch 0\n",
      "tensor([[20.5073]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[-51.7517]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[32.9655]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[23.4655]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[26.5001]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[15.9190]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[-41.8627]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[-7.7527]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[4.0758]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[16.2037]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 0 : 25.10700035095215\n",
      "Starting epoch 1\n",
      "tensor([[29.1800]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[-10.9095]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[72.9498]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[52.5395]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[53.3674]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[24.9685]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[-2.5401]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[-1.8689]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[8.8089]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[20.9796]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 1 : 10.723892211914062\n",
      "Starting epoch 2\n",
      "tensor([[20.3328]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[3.2420]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[54.1505]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[34.4822]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[49.6861]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[19.5496]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[2.3144]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[0.4408]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[8.3289]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[14.1590]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 2 : 9.404288291931152\n",
      "Starting epoch 3\n",
      "tensor([[18.3972]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[6.0093]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[82.9542]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[50.5367]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[65.5592]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[15.1896]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[3.1713]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[3.8886]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[8.2395]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[12.5134]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 3 : 7.0229692459106445\n",
      "Starting epoch 4\n",
      "tensor([[22.4404]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[3.4834]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[57.2683]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[44.8746]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[38.9569]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[14.6137]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[3.3612]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[3.5760]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[7.4756]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[12.6757]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 4 : 9.370674133300781\n",
      "Starting epoch 5\n",
      "tensor([[22.3818]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[1.0529]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[68.1180]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[68.2211]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[51.9430]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[17.4125]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[2.6064]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[1.5796]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[5.1200]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[14.1068]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 5 : 5.672837257385254\n",
      "Starting epoch 6\n",
      "tensor([[21.6167]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[0.7432]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[63.4441]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[69.1781]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[46.9473]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[17.7595]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[1.9987]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[2.0443]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[6.0435]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[19.1795]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 6 : 5.6783905029296875\n",
      "Starting epoch 7\n",
      "tensor([[20.8485]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[1.7924]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[64.1800]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[71.5140]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[51.5797]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[15.0430]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[2.2741]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[2.7722]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[4.7425]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[15.3927]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 7 : 5.36629056930542\n",
      "Starting epoch 8\n",
      "tensor([[16.8012]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[0.9285]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[56.4370]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[68.9667]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[49.1626]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[17.8055]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[1.7665]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[0.8087]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[5.8993]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[13.8563]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 8 : 6.038295269012451\n",
      "Starting epoch 9\n",
      "tensor([[15.7076]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[-0.4818]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[52.9162]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[70.8933]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[48.9391]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[16.0503]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[-0.3346]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[-1.1034]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[3.2538]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[8.3252]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 9 : 6.4328293800354\n",
      "Starting epoch 10\n",
      "tensor([[20.1636]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[0.8973]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[55.0519]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[69.4615]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[41.3817]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[16.9989]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[1.4069]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[1.8121]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[4.4543]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[11.4322]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 10 : 5.380308628082275\n",
      "Starting epoch 11\n",
      "tensor([[21.8318]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[1.1744]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[58.6588]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[80.3947]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[51.5554]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[17.9170]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[1.1454]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[1.7066]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[4.1459]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[12.8139]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 11 : 4.364490509033203\n",
      "Starting epoch 12\n",
      "tensor([[24.3945]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[1.0056]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[59.4081]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[67.0252]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[53.1918]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[26.4844]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[1.2625]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[1.3151]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[7.3472]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[21.3011]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 12 : 5.099492073059082\n",
      "Starting epoch 13\n",
      "tensor([[22.9779]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[3.1737]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[62.1530]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[68.8699]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[53.3289]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[21.6997]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[3.3479]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[2.6823]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[7.5961]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[14.9941]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 13 : 4.487001419067383\n",
      "Starting epoch 14\n",
      "tensor([[21.2847]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[-0.0314]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[63.8658]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[75.7241]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[51.2248]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[20.3785]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[0.5748]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[1.1239]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[5.3465]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[14.2542]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 14 : 4.525763034820557\n",
      "Starting epoch 15\n",
      "tensor([[29.5825]], device='cuda:0', grad_fn=<AddmmBackward>) 32\n",
      "tensor([[0.8291]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[62.0436]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[71.4021]], device='cuda:0', grad_fn=<AddmmBackward>) 85\n",
      "tensor([[51.6656]], device='cuda:0', grad_fn=<AddmmBackward>) 50\n",
      "tensor([[26.0378]], device='cuda:0', grad_fn=<AddmmBackward>) 19\n",
      "tensor([[0.7226]], device='cuda:0', grad_fn=<AddmmBackward>) 1\n",
      "tensor([[1.2268]], device='cuda:0', grad_fn=<AddmmBackward>) 2\n",
      "tensor([[7.6540]], device='cuda:0', grad_fn=<AddmmBackward>) 8\n",
      "tensor([[22.1171]], device='cuda:0', grad_fn=<AddmmBackward>) 18\n",
      "loss after epoch 15 : 4.65995979309082\n",
      "Starting epoch 16\n",
      "early stopping!\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'swa_model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-585c9253866a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mswa_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswa_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAveragedModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mswa_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswa_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSWALR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-00bcd0e4ef6e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mswa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data/output/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswa_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data/output/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'swa_model'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss_val\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'swa_model' is not defined"
     ]
    }
   ],
   "source": [
    "# adam works best with lr of 0.001 (tested 0.1 and 0.01)\n",
    "# adam without scheduler works best\n",
    "# first it took 6 minutes to load all the datasets. With lru cache it was immediate (6GB memory use). With disk cache it took about 1-2 minutes. Great result\n",
    "# feature extraction led to MSE of 12 after 40 epochs. Didnt really work. Maybe unfreeze more\n",
    "# adam was outperformed in feature extraction, but for finetuning it worked better\n",
    "# larger bs converges better and runs sligthly faster (about 10%) [512, 256,64,8]\n",
    "\n",
    "import adabound\n",
    "\n",
    "def mae_loss(y, pred):\n",
    "    return (torch.abs(y-pred.T)).mean()\n",
    "loss_fn = mae_loss\n",
    "\n",
    "NUM_EPOCH = 50\n",
    "SWA_START = 40\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 512\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SWA_ENABLED = False\n",
    "SCHED_ENABLED = False\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# xs = torch.load('data/input/xs').to(DEVICE)\n",
    "# ys = torch.load('data/input/ys').to(DEVICE)\n",
    "\n",
    "# train_set = AgeTensorDataset(xs,ys, valid=False)\n",
    "train_set = Ageset(\"data/face_age\")[:4000]\n",
    "determine_size(train_set)\n",
    "train_dl = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# val_set = AgeTensorDataset(xs,ys, valid=True)\n",
    "val_set = Ageset(\"data/face_age\", valid=True)[:1000]\n",
    "determine_size(val_set) \n",
    "val_dl = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dls = {'train': train_dl, 'val': val_dl}\n",
    "\n",
    "feat={True:'feat_ext', False:'finetune'}\n",
    "opts = {0:'adam',1:'adabound'}\n",
    "\n",
    "for i in [False]:\n",
    "    for j in range(1):\n",
    "        for BATCH_SIZE in [256]:\n",
    "            for res in [18,50,152]:\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "                writer = SummaryWriter(comment=f'{feat[i]} opt {opts[j]} epoch {NUM_EPOCH} SWA_START {SWA_START} LR BATCH_SIZE {LR}')\n",
    "                model = AgeResnet(size=str(res), feat_extract=i)\n",
    "                model = model.to(DEVICE)\n",
    "\n",
    "                if j ==0:\n",
    "                    opt = torch.optim.Adam(model.parameters(), LR)\n",
    "                if j ==1:\n",
    "                    opt = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
    "\n",
    "\n",
    "                if SCHED_ENABLED:\n",
    "                    # if i ==0:\n",
    "                    #     sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt)\n",
    "                    #     sched.get_last_lr = lambda: [1]\n",
    "                    # if i == 1:\n",
    "                    #     sched = torch.optim.lr_scheduler.OneCycleLR(opt, LR, steps_per_epoch=len(train_dl), epochs=NUM_EPOCH)\n",
    "                    # if i == 2:\n",
    "                    sched = torch.optim.lr_scheduler.MultiplicativeLR(opt, lr_lambda=lambda x: 1)\n",
    "                if SWA_ENABLED:\n",
    "                    swa_model = swa_utils.AveragedModel(model)\n",
    "                    swa_sched = swa_utils.SWALR(opt, swa_lr = 0.0005)\n",
    "                train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "model = AgeResnet()\n",
    "model.load_state_dict(torch.load(\"data/output/modeltensor(4.3645, device='cuda:0')\"))\n",
    "# for i in test_set[:10]:\n",
    "#     preds = i[1],model(i[0][None].to(DEVICE)).item()\n",
    "#     print(f'target {preds[0]}, predicted {preds[1]}')\n",
    "#     loss = abs(preds[0]-preds[1])\n",
    "#     # print('loss',loss)\n",
    "#     running += loss\n",
    "# print(running, running/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len validation dataset 5866\n",
      "items in dataset 1000 img_dimensions [3, 200, 200] size of ds in memory in gb: 0.4470348358154297\n",
      "tensor([[  5.4504],\n",
      "        [ 64.6927],\n",
      "        [ 40.4678],\n",
      "        [  2.7160],\n",
      "        [ 83.1447],\n",
      "        [ 43.6335],\n",
      "        [115.4910],\n",
      "        [108.6111]], grad_fn=<AddmmBackward>) tensor([ 5, 53, 15,  1, 60, 30, 80, 85])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(16.9009, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "val_set = Ageset(\"data/face_age\", valid=True)[:1000]\n",
    "determine_size(val_set) \n",
    "val_dl = DataLoader(val_set, batch_size=8, shuffle=True)\n",
    "x,y = next(iter(val_dl))\n",
    "x.to(DEVICE)\n",
    "model.eval()\n",
    "print (model(x),y)\n",
    "mae_loss(y, model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[39.5846]], grad_fn=<AddmmBackward>) 32\n",
      "tensor([[-0.8336]], grad_fn=<AddmmBackward>) 1\n",
      "tensor([[124.0162]], grad_fn=<AddmmBackward>) 50\n",
      "tensor([[168.4021]], grad_fn=<AddmmBackward>) 85\n",
      "tensor([[105.6732]], grad_fn=<AddmmBackward>) 50\n",
      "tensor([[32.9609]], grad_fn=<AddmmBackward>) 19\n",
      "tensor([[-0.9324]], grad_fn=<AddmmBackward>) 1\n",
      "tensor([[0.1149]], grad_fn=<AddmmBackward>) 2\n",
      "tensor([[4.1401]], grad_fn=<AddmmBackward>) 8\n",
      "tensor([[21.9747]], grad_fn=<AddmmBackward>) 18\n"
     ]
    }
   ],
   "source": [
    "for x,y in val_set[:10]:\n",
    "    model.eval()\n",
    "    x.to(DEVICE)\n",
    "    print(model(x[None]),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[3.3705]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "model(val_set[1][0][None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch tensorboard\n",
    "python -m tensorboard.main --logdir=runs --host=0.0.0.0 --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard dev upload --logdir runs \\\n",
    "--name \"My latest experiment\" \\ # optional\n",
    "--description \"Simple comparison of several hyperparameters\" # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'data/output/bestmodel.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda', index=0))"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "a = torch.Tensor([2])\n",
    "b = a.to(DEVICE)\n",
    "a.device, b.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
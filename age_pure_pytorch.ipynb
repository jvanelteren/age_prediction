{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('base': conda)",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler, swa_utils\n",
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "import time\n",
    "# from joblib import Memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache\n",
    "cache = Cache('.cache/diskcache')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Ageset(Dataset):\n",
    "\n",
    "    def __init__(self, path, transforms = None, valid=False, split_pct = 0.3):\n",
    "        self.image_paths = list(Path(path).rglob(\"*.png\"))\n",
    "        random.seed(42)\n",
    "        random.shuffle(self.image_paths)\n",
    "        split_point = int(len(self)*0.3)\n",
    "        if valid:\n",
    "            self.image_paths = self.image_paths[:split_point]\n",
    "            print('len validation dataset', len(self.image_paths))\n",
    "        else:\n",
    "            self.image_paths = self.image_paths[split_point:]\n",
    "            print('len train dataset', len(self.image_paths))\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def imgpath_to_normalized_tensor(self, imgpath):\n",
    "        # makes a tensor, scales range to 0-1 and normalizes to same as imagenet\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        return normalize(transforms.PILToTensor()(Image.open(imgpath)).float()/255)\n",
    "    \n",
    "    @cache.memoize(typed=True)\n",
    "    def __getitem__(self,i):\n",
    "        print(i)\n",
    "        # return self.image_paths[i]\n",
    "\n",
    "        if isinstance(i, slice):\n",
    "            return [self[n] for n,_ in enumerate(self.image_paths[i])]\n",
    "        \n",
    "        return (self.imgpath_to_normalized_tensor(self.image_paths[i]),\n",
    "                int(self.image_paths[i].parent.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len train dataset 13690\n"
     ]
    }
   ],
   "source": [
    "a = Ageset(\"data/face_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "40\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[ 2.1975,  2.2147,  2.2318,  ..., -0.0972, -0.1143, -0.0972],\n",
       "          [ 2.1975,  2.2147,  2.2318,  ..., -0.1314, -0.1657, -0.1486],\n",
       "          [ 2.1975,  2.1975,  2.2147,  ..., -0.1657, -0.2171, -0.1999],\n",
       "          ...,\n",
       "          [-1.4843, -1.3987, -1.3987,  ..., -1.6384, -1.6042, -1.6042],\n",
       "          [-1.4329, -1.2959, -1.2959,  ..., -1.6727, -1.7925, -1.8782],\n",
       "          [-1.4329, -1.2788, -1.2445,  ..., -1.6727, -1.7754, -1.8439]],\n",
       " \n",
       "         [[ 2.1134,  2.1134,  2.1310,  ..., -1.0728, -1.0728, -1.0028],\n",
       "          [ 2.1134,  2.1134,  2.0959,  ..., -1.1078, -1.1253, -1.0553],\n",
       "          [ 2.0784,  2.0959,  2.0784,  ..., -1.1429, -1.1779, -1.1078],\n",
       "          ...,\n",
       "          [-1.3004, -1.2129, -1.2129,  ..., -1.4230, -1.3880, -1.3880],\n",
       "          [-1.2479, -1.1078, -1.1078,  ..., -1.4230, -1.5455, -1.6331],\n",
       "          [-1.2479, -1.0903, -1.0553,  ..., -1.4230, -1.5280, -1.5980]],\n",
       " \n",
       "         [[ 1.9603,  1.9254,  1.9428,  ..., -1.1073, -1.0724, -1.0201],\n",
       "          [ 1.9603,  1.9254,  1.9254,  ..., -1.1421, -1.1247, -1.0724],\n",
       "          [ 1.9428,  1.9080,  1.9080,  ..., -1.1770, -1.1770, -1.1247],\n",
       "          ...,\n",
       "          [-1.0027, -0.9156, -0.9156,  ..., -1.0898, -1.0550, -1.0550],\n",
       "          [-0.9504, -0.8110, -0.8110,  ..., -1.0724, -1.1944, -1.2816],\n",
       "          [-0.9504, -0.7936, -0.7587,  ..., -1.0724, -1.1770, -1.2467]]]),\n",
       " 1)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "a[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Ageset(\"data/face_age\", valid=True)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adaptedRes(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(adaptedRes, self).__init__()\n",
    "        resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        modules=list(resnet.children())[:-1]\n",
    "        self.resnet =nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(in_features=512, out_features=1, bias=True)\n",
    "\n",
    "        # for m in self.modules():\n",
    "        # if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(self.fc.weight)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.resnet(x)\n",
    "        x = torch.flatten(out, 1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss = {'train':[], 'val':[]}\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        print(f'Starting epoch {epoch}')\n",
    "        start_time = time.time()\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            total_loss = 0\n",
    "            for data in dls[phase]:\n",
    "                x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    pred = model(x)\n",
    "                    loss = loss_fn(y, pred)\n",
    "                    total_loss += loss * len(y)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        opt.step()\n",
    "                        opt.zero_grad()\n",
    "                        if SWA_ENABLED and epoch > SWA_START:\n",
    "                            swa_model.update_parameters(model)\n",
    "                            swa_sched.step()\n",
    "                        elif SCHED_ENABLED:\n",
    "                            sched.step(loss)\n",
    "                            writer.add_scalar('lr/scheduler', sched.get_last_lr()[0], epoch)\n",
    "                        writer.add_scalar('lr/optparamgroup0', opt.param_groups[0]['lr'], epoch)\n",
    "                writer.add_scalar('batchloss/'+phase, loss, epoch)\n",
    "            \n",
    "            writer.add_scalar('loss/'+phase, total_loss/len(dls[phase].dataset), epoch)\n",
    "            \n",
    "        print(f\"loss after epoch {epoch} : {total_loss / len(dls['val'].dataset)}\")\n",
    "        writer.add_scalar('time', (time.time()-start_time)/60, epoch)\n",
    "\n",
    "\n",
    "    if SWA_ENABLED:\n",
    "        # swa_utils.update_bn(train_dl, swa_model)\n",
    "        total_loss_train = 0\n",
    "        total_loss_val = 0\n",
    "        with torch.no_grad():\n",
    "            for data in train_dl:\n",
    "                    x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                    total_loss_train += loss_fn(y, model(x)) * len(y)\n",
    "            writer.add_scalar('loss/train', total_loss_train/len(train_set), epoch+1)\n",
    "\n",
    "            for data in val_dl:\n",
    "                    x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                    total_loss_val += loss_fn(y, model(x)) * len(y)\n",
    "            writer.add_scalar('loss/val', total_loss_train/len(val_set), epoch+1)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "len train dataset 13690\n",
      "len validation dataset 5866\n",
      "Starting epoch 0\n",
      "loss after epoch 0 : 8.340116500854492\n",
      "Starting epoch 1\n",
      "loss after epoch 1 : 9.198281288146973\n",
      "Starting epoch 2\n",
      "loss after epoch 2 : 8.051300048828125\n",
      "Starting epoch 3\n",
      "loss after epoch 3 : 5.771472454071045\n",
      "Starting epoch 4\n",
      "loss after epoch 4 : 5.379964351654053\n",
      "Starting epoch 5\n",
      "loss after epoch 5 : 6.048651218414307\n",
      "Starting epoch 6\n",
      "loss after epoch 6 : 5.4881391525268555\n",
      "Starting epoch 7\n",
      "loss after epoch 7 : 4.725648880004883\n",
      "Starting epoch 8\n",
      "loss after epoch 8 : 5.262847423553467\n",
      "Starting epoch 9\n",
      "loss after epoch 9 : 4.763493061065674\n",
      "Starting epoch 10\n",
      "loss after epoch 10 : 4.864866733551025\n",
      "Starting epoch 11\n",
      "loss after epoch 11 : 4.70595121383667\n",
      "Starting epoch 12\n",
      "loss after epoch 12 : 4.754103660583496\n",
      "Starting epoch 13\n",
      "loss after epoch 13 : 5.09583854675293\n",
      "Starting epoch 14\n",
      "loss after epoch 14 : 4.579208850860596\n",
      "Starting epoch 15\n",
      "loss after epoch 15 : 4.413903713226318\n",
      "Starting epoch 16\n",
      "loss after epoch 16 : 5.756808757781982\n",
      "Starting epoch 17\n",
      "loss after epoch 17 : 4.713752746582031\n",
      "Starting epoch 18\n",
      "loss after epoch 18 : 4.519155025482178\n",
      "Starting epoch 19\n",
      "loss after epoch 19 : 4.208810806274414\n"
     ]
    }
   ],
   "source": [
    "def mae_loss(y, pred):\n",
    "    return (torch.abs(y-pred.T)).mean()\n",
    "loss_fn = mae_loss\n",
    "\n",
    "NUM_EPOCH = 20\n",
    "SWA_START = 20\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SWA_ENABLED = False\n",
    "SCHED_ENABLED = True\n",
    "\n",
    "train_set = Ageset(\"data/face_age\")[:4000]\n",
    "train_dl = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_set = Ageset(\"data/face_age\", valid=True)[:1000]\n",
    "val_dl = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dls = {'train': train_dl, 'val': val_dl}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2,3):\n",
    "\n",
    "    runs = {0:'ReduceLROnPlateau', 1:'OneCycleLR', 2:'MultiplicativeLR'}\n",
    "\n",
    "    writer = SummaryWriter(comment=f'{runs[i]} epoch {NUM_EPOCH} SWA_START {SWA_START} LR BATCH_SIZE {LR}')\n",
    "    model = adaptedRes()\n",
    "    model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), LR)\n",
    "    if SCHED_ENABLED:\n",
    "        if i ==0:\n",
    "            sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt)\n",
    "            sched.get_last_lr = lambda: [1]\n",
    "        if i == 1:\n",
    "            sched = torch.optim.lr_scheduler.OneCycleLR(opt, LR, steps_per_epoch=len(train_dl), epochs=NUM_EPOCH)\n",
    "        if i == 2:\n",
    "            sched = torch.optim.lr_scheduler.MultiplicativeLR(opt, lr_lambda=lambda x: 1)\n",
    "    if SWA_ENABLED:\n",
    "        swa_model = swa_utils.AveragedModel(model)\n",
    "        swa_sched = swa_utils.SWALR(opt, swa_lr = 0.0005)\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'ReduceLROnPlateau' object has no attribute 'lr'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f72f1d33cd0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReduceLROnPlateau' object has no attribute 'lr'"
     ]
    }
   ],
   "source": [
    "sched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target 16, predicted -0.47240400314331055\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'running' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-63bee9934c35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# print('loss',loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mrunning\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'running' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in test_set[:10]:\n",
    "    preds = i[1],model(i[0][None].to(DEVICE)).item()\n",
    "    print(f'target {preds[0]}, predicted {preds[1]}')\n",
    "    loss = abs(preds[0]-preds[1])\n",
    "    # print('loss',loss)\n",
    "    running += loss\n",
    "print(running, running/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'tensorboard.main' is not recognized as an internal or external command,\noperable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!tensorboard.main --logdir='runs' ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.2206122080485026"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "(a-time.time())/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch tensorboard\n",
    "python -m tensorboard.main --logdir=runs --host=0.0.0.0 --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard dev upload --logdir runs \\\n",
    "--name \"My latest experiment\" \\ # optional\n",
    "--description \"Simple comparison of several hyperparameters\" # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-42-fcd86dbf9b07>, line 6)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-fcd86dbf9b07>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    def train(self:)\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Learner():\n",
    "    def __init__(self, dls, model, opt, metric, parameters, sched=None, swa_model=None, swa_sched=None):\n",
    "        self.dls, self.model, self.opt, self.metric, self.p = dls, model, opt, metric, parameters\n",
    "        self.sched, self.swa_model, self.swa_sched = sched, swa_model, swa_sched\n",
    "        \n",
    "    def train(self:)\n",
    "        for epoch in range(NUM_EPOCH):\n",
    "                print('epoch start')\n",
    "                self.model.train()\n",
    "                total_loss_train = 0\n",
    "                for data in self.train_dl:\n",
    "                    x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                    opt.zero_grad()\n",
    "                    pred = model(x)\n",
    "                    loss = loss_fn(y, pred)\n",
    "                    total_loss_train += loss * len(y)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    if epoch > SWA_START:\n",
    "                        swa_model.update_parameters(model)\n",
    "                        swa_sched.step(loss)\n",
    "                    else:\n",
    "                        sched.step()\n",
    "                \n",
    "                model.eval()\n",
    "                total_loss_test = 0\n",
    "                with torch.no_grad():\n",
    "                    for data in test_dl:\n",
    "                        x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                        total_loss_test += loss_fn(y, model(x)) * len(y)\n",
    "                register_runs(total_loss_train, total_loss_test, epoch)\n",
    "\n",
    "    def register_runs(total_loss_train, total_loss_test, epoch): \n",
    "        writer.add_scalar('loss/train', total_loss_train/len(train_set), epoch)\n",
    "        writer.add_scalar('loss/test', total_loss_test/len(test_set), epoch)\n",
    "        writer.add_scalar('lr/optparamgroup0', opt.param_groups[0]['lr'], epoch)\n",
    "        writer.add_scalar('lr/scheduler', sched.get_last_lr()[0], epoch)\n",
    "        print(f'loss after epoch {epoch}: {total_loss_train/len(train_set)}, {total_loss_test/len(test_set)}')\n",
    "\n",
    "    def validate_swa_model(swa_model):\n",
    "        # swa_utils.update_bn(train_dl, swa_model)\n",
    "        total_loss_train = 0\n",
    "            total_loss_test = 0\n",
    "            with torch.no_grad():\n",
    "            for data in train_dl:\n",
    "                    x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                    total_loss_train += loss_fn(y, model(x)) * len(y)\n",
    "            writer.add_scalar('loss/train', total_loss_train/len(train_set), epoch+1)\n",
    "\n",
    "            for data in test_dl:\n",
    "                    x, y = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "                    total_loss_test += loss_fn(y, model(x)) * len(y)\n",
    "            writer.add_scalar('loss/test', total_loss_train/len(test_set), epoch+1)\n",
    "\n",
    "            writer.flush()\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}